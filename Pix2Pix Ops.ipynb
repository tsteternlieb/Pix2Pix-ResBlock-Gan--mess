{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "import time\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResBlock(layers.Layer):\n",
    "    def __init__(self,kernel_sizes = 3, **kwargs):\n",
    "        super(ResBlock, self).__init__(**kwargs)\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        channels = input_shape[-1]\n",
    "        self.relU_1 = layers.ReLU()\n",
    "        self.bn_1 = layers.BatchNormalization()\n",
    "        self.conv2d_1 = layers.Conv2D(channels, 3, padding = 'same')\n",
    "        self.relU_2 = layers.ReLU()\n",
    "        self.bn_2 = layers.BatchNormalization()\n",
    "        self.conv2d_2 = layers.Conv2D(channels, 3, padding = 'same')\n",
    "    def call(self, inputs, training = True):\n",
    "        x = self.relU_1(inputs)\n",
    "        x = self.bn_1(inputs, training)\n",
    "        x = self.conv2d_1(x)\n",
    "        x = self.relU_2(x)\n",
    "        x = self.bn_2(x)\n",
    "        x = self.conv2d_2(x)\n",
    "        out = inputs + x\n",
    "        return out\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneByOne_Scale(layers.Layer):\n",
    "    def __init__(self, layer_num, scale_down = True, factor = 2, **kwargs):\n",
    "        super(OneByOne_Scale, self).__init__(**kwargs)\n",
    "        self.layer_num = layer_num\n",
    "        self.scale_down = scale_down\n",
    "        self.factor = factor\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        if self.scale_down == True:\n",
    "            self.scale = layers.MaxPool2D(self.factor)\n",
    "        else:\n",
    "            self.scale = layers.UpSampling2D(interpolation = \"nearest\")\n",
    "        self.OneByOne = layers.Conv2D(self.layer_num,3, padding = \"same\")\n",
    "        self.ReLu = layers.ReLU()\n",
    "    def output_shape(self, inputs):\n",
    "        return inputs.shape\n",
    "    def call(self, inputs):\n",
    "        x = self.scale(inputs)\n",
    "        x = self.OneByOne(x)\n",
    "        x = self.ReLu(x)\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNET(keras.Model):\n",
    "    def __init__(self, optimizer):\n",
    "        super(UNET, self).__init__()\n",
    "        \n",
    "        self.optimizer = optimizer \n",
    "        \n",
    "        self.Expand = layers.Conv2D(64,(1,1),activation = \"relu\")\n",
    "        self.OneByOne1 = OneByOne_Scale(128)\n",
    "        self.OneByOne2 = OneByOne_Scale(256)\n",
    "        self.OneByOne3 = OneByOne_Scale(512)\n",
    "        self.OneByOne4 = OneByOne_Scale(1024)\n",
    "        \n",
    "        self.OneByOne5 = OneByOne_Scale(512, False)\n",
    "        self.OneByOne6 = OneByOne_Scale(256, False)\n",
    "        self.OneByOne7 = OneByOne_Scale(128, False)\n",
    "        self.OneByOne8 = OneByOne_Scale(64, False)\n",
    "        \n",
    "        self.ResBlock1 = ResBlock()\n",
    "        self.ResBlock2 = ResBlock()\n",
    "        self.ResBlock3 = ResBlock()\n",
    "        self.ResBlock4 = ResBlock()\n",
    "        self.ResBlock5 = ResBlock()\n",
    "        \n",
    "        self.ResBlock6 = ResBlock()\n",
    "        self.ResBlock7 = ResBlock()\n",
    "        self.ResBlock8 = ResBlock()\n",
    "        self.ResBlock9 = ResBlock()\n",
    "        \n",
    "        self.FinalOneByOne = layers.Conv2D(kernel_size = (1,1), filters = 3, activation = 'sigmoid', padding = 'same')\n",
    "\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        \n",
    "        channel_upsample64 = self.Expand(inputs)\n",
    "        x1 = self.ResBlock1(channel_upsample64)\n",
    "        x2 = self.OneByOne1(x1)\n",
    "        x2 = layers.Dropout(50)(self.ResBlock2(x2))\n",
    "        x3 = self.OneByOne2(x2)\n",
    "        x3 = self.ResBlock3(x3)\n",
    "        #[batch, 128,128,256]\n",
    "        \n",
    "        x4 = self.OneByOne3(x3)\n",
    "        x4 = self.ResBlock4(x4)\n",
    "        #[batch, 64,64,512]\n",
    "        \n",
    "\n",
    "        x5 = self.OneByOne4(x4)\n",
    "        x5 = self.ResBlock5(x5)\n",
    "        #[batch,32,32,1024]\n",
    "        \n",
    "        \n",
    "        x4_up = self.OneByOne5(x5)\n",
    "        x4_concat = tf.concat([x4,x4_up], axis = -1)        \n",
    "        x4_concat = self.ResBlock6(x4_concat)\n",
    "        \n",
    "        \n",
    "        x3_up = self.OneByOne6(x4_concat)\n",
    "        x3_concat = tf.concat([x3,x3_up], axis = -1)\n",
    "        x3_concat = self.ResBlock7(x3_concat)\n",
    "        \n",
    "     \n",
    "        \n",
    "        x2_up = self.OneByOne7(x3_concat)\n",
    "        x2_concat = tf.concat([x2,x2_up], axis = -1)\n",
    "        x2_concat = self.ResBlock8(x2_concat)\n",
    "        \n",
    "        \n",
    "        x1_up = self.OneByOne8(x2_concat)\n",
    "        x1_concat = tf.concat([x1,x1_up], axis = -1)\n",
    "        x1_concat = self.ResBlock9(x1_concat)\n",
    "        \n",
    "        out = self.FinalOneByOne(x1_concat)\n",
    "        return out\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchDiscriminator(keras.Model):\n",
    "    def __init__(self,patch_size):\n",
    "        super(PatchDiscriminator, self).__init__()\n",
    "        self.patch_size = patch_size\n",
    "        \n",
    "        self.InitialConv2D = layers.Conv2D(kernel_size = (7,7), filters = 64, activation = 'relu', padding = 'same')\n",
    "        self.FinalConv = layers.Conv2D(kernel_size = 3, filters = 1, activation = 'sigmoid', padding = 'same')\n",
    "        \n",
    "        self.ResBlock1 = ResBlock()\n",
    "        self.ResBlock2 = ResBlock()\n",
    "        self.ResBlock3 = ResBlock()\n",
    "        self.ResBlock4 = ResBlock()\n",
    "        self.ResBlock5 = ResBlock()\n",
    "        \n",
    "        \n",
    "    def call(self, inputs):\n",
    "        x = self.InitialConv2D(inputs)\n",
    "        x = self.ResBlock1(x)\n",
    "        x = self.ResBlock2(x)\n",
    "        x = self.ResBlock3(x)\n",
    "        x = self.ResBlock4(x)\n",
    "        x = self.ResBlock5(x)\n",
    "        \n",
    "        x = self.FinalConv(x)\n",
    "        x = tf.reduce_mean(x,axis=[1,2,3])\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_layer = layers.ConvLSTM2D(filters = 64, kernel_size = (3,3), return_sequences = True)\n",
    "# x = tf.random.normal((4,2,16,16,3))\n",
    "# test_layer(x)\n",
    "class PatchDiscriminatorLSTM(keras.Model):\n",
    "    def __init__(self, optimizer, patch_size):\n",
    "        super(PatchDiscriminatorLSTM, self).__init__()\n",
    "        \n",
    "        self.optimizer = optimizer\n",
    "        self.patch_size = patch_size\n",
    "        \n",
    "        self.ConvLSTM1 = layers.ConvLSTM2D(filters = 64, padding = 'valid', strides = 3, kernel_size = 7, return_sequences = True)\n",
    "        self.ConvLSTM2 = layers.ConvLSTM2D(filters = 128, padding = 'valid', strides = 3, kernel_size = 5, return_sequences = True)\n",
    "        self.ConvLSTM3 = layers.ConvLSTM2D(filters = 256, padding = 'valid', strides = 3, kernel_size = 3, return_sequences = False)\n",
    "        self.Conv1 = layers.Conv2D(filters = 64, padding = 'same', strides = 3, kernel_size = 3, activation = 'relu')\n",
    "        self.Conv2 = layers.Conv2D(filters = 1, padding = 'same', strides = 3, kernel_size = self.patch_size, activation = 'sigmoid')\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        x = self.ConvLSTM1(inputs)\n",
    "        x = self.ConvLSTM2(x)\n",
    "        x = self.ConvLSTM3(x)\n",
    "        x = self.Conv1(x)\n",
    "        x = self.Conv2(x) \n",
    "        out = tf.reduce_mean(x,axis=[1,2,3])\n",
    "        return out\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "def discriminator_loss(real_output, fake_output):\n",
    "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
    "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
    "    total_loss = real_loss + fake_loss \n",
    "    return total_loss, real_loss, fake_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_loss(disc_decision, fake_output, ground_truth, alpha):\n",
    "    l1_loss = tf.reduce_mean(tf.abs(fake_output-ground_truth))\n",
    "    gen_loss = cross_entropy(tf.ones_like(disc_decision), disc_decision)\n",
    "    total_loss = gen_loss + alpha * l1_loss\n",
    "    return total_loss, gen_loss, l1_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BatchGenerator(batch_size, im_dir, look_back, height, width):\n",
    "    photo_dir = glob.glob(im_dir + \"/*.jpg\")\n",
    "    while True:\n",
    "        photo_pairs = np.zeros((batch_size,look_back,height, width, 3)) \n",
    "        for i in range(batch_size):\n",
    "            photo1_id = random.randint(look_back,len(photo_dir)-3)\n",
    "            \n",
    "            id1 = f'{photo1_id:03d}'\n",
    "            id2 = f'{photo1_id+1:03d}'\n",
    "            \n",
    "            photo1_path = im_dir + '/0' + id1 + '.jpg'\n",
    "            photo2_path = im_dir + '/0' + id2 + '.jpg'            \n",
    "            \n",
    "            photo1_array = (np.asarray(Image.open(photo1_path).resize((height,width)))-127.5)/127.5\n",
    "            photo2_array = (np.asarray(Image.open(photo2_path).resize((height,width)))-127.5)/127.5\n",
    "            \n",
    "            photo_pairs[i,0] = photo1_array\n",
    "            photo_pairs[i,1] = photo2_array\n",
    "        yield tf.cast(photo_pairs, tf.float32)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = BatchGenerator(6,'./NatureTrim', 2, 512,512).__next__()\n",
    "a = batch[1,0]\n",
    "PIL_image = Image.fromarray(np.uint8((127.5*a)+127.5)).convert('RGB')\n",
    "PIL_image.save(\"frea.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(batch_generator, gen, disc, batch_size):\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "        d_train = batch_generator.__next__()\n",
    "        g_train = batch_generator.__next__()\n",
    "        \n",
    "        #calculate Generator Loss\n",
    "        g_images_for_d = gen(d_train[:batch_size//2,0])\n",
    "        g_images_for_d = tf.expand_dims(g_images_for_d,1)       \n",
    "    \n",
    "\n",
    "        d_train = tf.expand_dims(d_train[:batch_size//2,0], 1)\n",
    "        print(d_train.shape, g_images_for_d.dtype)\n",
    "        \n",
    "        \n",
    "        g_concat = tf.concat([d_train, g_images_for_d], axis = 1)\n",
    "        fake_pred = disc(g_concat)\n",
    "        \n",
    "        real_pred = disc(d_train)\n",
    "        \n",
    "        disc_loss, disc_real_loss, disc_fake_loss = discriminator_loss(real_pred, fake_pred)\n",
    "\n",
    "        #calculate Discriminator Loss\n",
    "        \n",
    "        fake_image = gen(g_train[:,0])\n",
    "        fake_image = tf.expand_dims(fake_image,1)\n",
    "        g_train_temp = tf.expand_dims(g_train[:,0], axis = 1)\n",
    "        fake_pred_for_gen = disc(tf.concat([g_train_temp,fake_image],axis = 1))                      \n",
    "        gen_loss, gen_disc_loss, l1_loss = generator_loss(fake_pred_for_gen,fake_image, g_train[:1], 1)\n",
    "        \n",
    "\n",
    "        \n",
    "    gradients_of_generator = gen_tape.gradient(gen_loss, gen.trainable_variables)\n",
    "    gradients_of_discriminator = disc_tape.gradient(disc_loss, disc.trainable_variables)\n",
    "        \n",
    "    gen.optimizer.apply_gradients(zip(gradients_of_generator, gen.trainable_variables))\n",
    "    disc.optimizer.apply_gradients(zip(gradients_of_discriminator, disc.trainable_variables))\n",
    "    \n",
    "    return (disc_real_loss, disc_fake_loss, gen_disc_loss, l1_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 2\n",
    "BATCH_GENERATOR = BatchGenerator(BATCH_SIZE,'./NatureTrim', 2, 512,512)\n",
    "EPOCHS = 6\n",
    "STEPS_PER_EPOCH = 12\n",
    "OPTIMIZER = keras.optimizers.Adam(1e-4)\n",
    "GENERATOR = UNET(OPTIMIZER)\n",
    "DISCRIMINATOR = PatchDiscriminatorLSTM(OPTIMIZER,70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(Generator, Discriminator, batch_size, epochs, steps_per_epoch, batch_generator):\n",
    "    checkpoint_dir = './training_checkpoints'\n",
    "    checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "    checkpoint = tf.train.Checkpoint(generator_optimizer=Generator.optimizer,\n",
    "                                 discriminator_optimizer=Discriminator.optimizer,\n",
    "                                 generator=Generator,\n",
    "                                 discriminator=Discriminator)\n",
    "    seed = batch_generator.__next__()[0:1,0]\n",
    "    \n",
    "    disc_real_loss_hist = []\n",
    "    disc_fake_loss_hist = []\n",
    "    gen_loss_1_hist = []\n",
    "    gen_loss_2_hist = []\n",
    "    \n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        start = time.time()\n",
    "        \n",
    "        for step in range(steps_per_epoch):\n",
    "            losses = train_step(batch_generator,Generator,Discriminator,batch_size)\n",
    "            print('disc real loss = %.8f disc fake loss %.8f gen loss 1 %.8f gen loss 2 %.8f' %\\\n",
    "                  (losses[0].numpy(), losses[1].numpy(), losses[2].numpy(), losses[3].numpy()))\n",
    "            \n",
    "            \n",
    "            \n",
    "            disc_real_loss_hist.append([epoch, losses[0].numpy()])\n",
    "            disc_fake_loss_hist.append([epoch, losses[1].numpy()])\n",
    "            gen_loss_1_hist.append([epoch, losses[2].numpy()])\n",
    "            gen_loss_2_hist.append([epoch, losses[3].numpy()])\n",
    "            \n",
    "            \n",
    "        \n",
    "        print(\"time for epoch %d is %.2f\" % (epoch, time.time()-start))\n",
    "        \n",
    "        generate_and_save_images(Generator,\n",
    "                             epoch + 1,\n",
    "                             seed)\n",
    "        \n",
    "        if (epoch%15 == 1):\n",
    "            \n",
    "            plotter(np.asarray(disc_real_loss_hist),np.asarray(disc_fake_loss_hist),\n",
    "                   np.asarray(gen_loss_1_hist), np.asarray(gen_loss_2_hist), epoch)\n",
    "            \n",
    "            checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "        \n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_save_images(model, epoch, test_input):\n",
    "\n",
    "    predictions = model(test_input, training=False)\n",
    "    np.save(\"ims_at\" + str(epoch), predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotter(a1,a2,a3,a4, epoch):\n",
    "    plt.plot(a1[:,0],a1[:,1], alpha=0.7, label = 'disc_real_loss')\n",
    "    plt.plot(a2[:,0],a2[:,1], alpha=0.7, label = 'disc_fake_loss')\n",
    "    plt.plot(a3[:,0],a3[:,1], alpha=0.7, label = 'gen_loss_1')\n",
    "    plt.plot(a4[:,0],a4[:,1], alpha=0.7, label = 'gen_loss_2')\n",
    "    plt.legend()\n",
    "    plt.savefig(\"losses_\" + str(epoch), bbox_inches='tight', dpi=300)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1, 512, 512, 3) <dtype: 'float32'>\n",
      "(1, 1, 512, 512, 3) <dtype: 'float32'>\n",
      "disc real loss = 0.47407639 disc fake loss 0.97407824 gen loss 1 0.47407472 gen loss 2 0.89315826\n",
      "time for epoch 0 is 74.19\n",
      "disc real loss = 0.47408423 disc fake loss 0.97405726 gen loss 1 0.47408524 gen loss 2 0.51446813\n",
      "time for epoch 1 is 48.20\n",
      "disc real loss = 0.47409499 disc fake loss 0.97403920 gen loss 1 0.47409630 gen loss 2 0.32106903\n",
      "time for epoch 2 is 54.40\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-81-604acc3262a0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mDISCRIMINATOR\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPatchDiscriminatorLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mOPTIMIZER\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m70\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mtrain_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mGENERATOR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDISCRIMINATOR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEPOCHS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSTEPS_PER_EPOCH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBATCH_GENERATOR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-78-dc95bfb6c638>\u001b[0m in \u001b[0;36mtrain_loop\u001b[0;34m(Generator, Discriminator, batch_size, epochs, steps_per_epoch, batch_generator)\u001b[0m\n\u001b[1;32m     36\u001b[0m         generate_and_save_images(Generator,\n\u001b[1;32m     37\u001b[0m                              \u001b[0mepoch\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m                              seed)\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;36m15\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-79-fee675f834cb>\u001b[0m in \u001b[0;36mgenerate_and_save_images\u001b[0;34m(model, epoch, test_input)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mgenerate_and_save_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ims_at\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/tf_working2/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1010\u001b[0m         with autocast_variable.enable_auto_cast_variables(\n\u001b[1;32m   1011\u001b[0m             self._compute_dtype_object):\n\u001b[0;32m-> 1012\u001b[0;31m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1013\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-59-05b8c9b8a996>\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0mx4_up\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOneByOne5\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0mx4_concat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx4_up\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0mx4_concat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mResBlock6\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx4_concat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/tf_working2/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1010\u001b[0m         with autocast_variable.enable_auto_cast_variables(\n\u001b[1;32m   1011\u001b[0m             self._compute_dtype_object):\n\u001b[0;32m-> 1012\u001b[0;31m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1013\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-302df9cfadfd>\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, training)\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelU_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2d_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelU_2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn_2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/tf_working2/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1010\u001b[0m         with autocast_variable.enable_auto_cast_variables(\n\u001b[1;32m   1011\u001b[0m             self._compute_dtype_object):\n\u001b[0;32m-> 1012\u001b[0;31m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1013\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/tf_working2/lib/python3.7/site-packages/tensorflow/python/keras/layers/convolutional.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m       \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compute_causal_padding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 248\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convolution_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_bias\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/tf_working2/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/tf_working2/lib/python3.7/site-packages/tensorflow/python/ops/nn_ops.py\u001b[0m in \u001b[0;36mconvolution_v2\u001b[0;34m(input, filters, strides, padding, data_format, dilations, name)\u001b[0m\n\u001b[1;32m   1018\u001b[0m       \u001b[0mdata_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1019\u001b[0m       \u001b[0mdilations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdilations\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1020\u001b[0;31m       name=name)\n\u001b[0m\u001b[1;32m   1021\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1022\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/tf_working2/lib/python3.7/site-packages/tensorflow/python/ops/nn_ops.py\u001b[0m in \u001b[0;36mconvolution_internal\u001b[0;34m(input, filters, strides, padding, data_format, dilations, name, call_from_convolution, num_spatial_dims)\u001b[0m\n\u001b[1;32m   1148\u001b[0m           \u001b[0mdata_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1149\u001b[0m           \u001b[0mdilations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdilations\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1150\u001b[0;31m           name=name)\n\u001b[0m\u001b[1;32m   1151\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1152\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mchannel_index\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/tf_working2/lib/python3.7/site-packages/tensorflow/python/ops/nn_ops.py\u001b[0m in \u001b[0;36m_conv2d_expanded_batch\u001b[0;34m(input, filters, strides, padding, data_format, dilations, name)\u001b[0m\n\u001b[1;32m   2602\u001b[0m         \u001b[0mdata_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2603\u001b[0m         \u001b[0mdilations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdilations\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2604\u001b[0;31m         name=name)\n\u001b[0m\u001b[1;32m   2605\u001b[0m   return squeeze_batch_dims(\n\u001b[1;32m   2606\u001b[0m       \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/tf_working2/lib/python3.7/site-packages/tensorflow/python/ops/gen_nn_ops.py\u001b[0m in \u001b[0;36mconv2d\u001b[0;34m(input, filter, strides, padding, use_cudnn_on_gpu, explicit_paddings, data_format, dilations, name)\u001b[0m\n\u001b[1;32m    937\u001b[0m           \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrides\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstrides\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    938\u001b[0m           \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexplicit_paddings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexplicit_paddings\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 939\u001b[0;31m           data_format=data_format, dilations=dilations, name=name, ctx=_ctx)\n\u001b[0m\u001b[1;32m    940\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_SymbolicException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    941\u001b[0m       \u001b[0;32mpass\u001b[0m  \u001b[0;31m# Add nodes to the TensorFlow graph.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/tf_working2/lib/python3.7/site-packages/tensorflow/python/ops/gen_nn_ops.py\u001b[0m in \u001b[0;36mconv2d_eager_fallback\u001b[0;34m(input, filter, strides, padding, use_cudnn_on_gpu, explicit_paddings, data_format, dilations, name, ctx)\u001b[0m\n\u001b[1;32m   1024\u001b[0m   explicit_paddings, \"data_format\", data_format, \"dilations\", dilations)\n\u001b[1;32m   1025\u001b[0m   _result = _execute.execute(b\"Conv2D\", 1, inputs=_inputs_flat, attrs=_attrs,\n\u001b[0;32m-> 1026\u001b[0;31m                              ctx=ctx, name=name)\n\u001b[0m\u001b[1;32m   1027\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0m_execute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmust_record_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m     _execute.record_gradient(\n",
      "\u001b[0;32m/opt/anaconda3/envs/tf_working2/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/d3fzzAAAACXBIWXMAAAsTAAALEwEAmpwYAAAsyElEQVR4nO3deXRU9f3/8ecnM9lnJgkQFgkGUBKWsEMAgUBYsiqLWpdSFTe+SrX99vxstYtoK7RuX1r9auVQt6/Vqq2KWyYLYQsgIAmLsgYElQCFsGVmsmfy+f2RSCMGmMAkNzN5P87hnMzcT+59f5Lw4vLJve+rtNYIIYTwfQFGFyCEEMI7JNCFEMJPSKALIYSfkEAXQgg/IYEuhBB+wmzUgbt06aJ79+5t1OGFEMInFRUVndBaRze3zbBA7927N4WFhUYdXgghfJJS6pvzbZMlFyGE8BMS6EII4Sck0IUQwk9IoAshhJ+QQBdCCD8hgS6EEH5CAl0IIfyEYdehX7Iz38K3G42uQgghLl10f+gxxOu79b1ALzsMOz4wugrh1+QZAaKVDZwpgQ5A7LiGP0IIIb5H1tCFEMJPSKALIYSfkEAXQgg/IYEuhBB+QgJdCCH8hAS6EEL4CQl0IYTwExLoQgjhJyTQhRDCT0igCyGEn5BAF0IIPyGBLoQQfkICXQgh/IQEuhBC+AkJdCGE8BMS6EII4Sck0IUQwk9IoAshhJ+QQBdCCD8hgS6EEH5CAl0IIfyEBLoQQvgJCXQhhPATEuhCCOEnPAp0pVSaUmqvUmq/UuqRZrZHKaWWKaW+UEp9rpRK8H6pQgghLuSiga6UMgEvAunAQOBWpdTAc4b9BtimtR4C3A485+1ChRBCXJgnZ+iJwH6t9QGtdQ3wDjDznDEDgRUAWus9QG+lVDevViqEEOKCPAn0nsChJq9LGt9rajtwPYBSKhGIBWK8UaAQQgjPeBLoqpn39DmvnwSilFLbgAeBrUDdD3ak1DylVKFSqrC0tLSltQohhLgATwK9BOjV5HUMcKTpAK21Q2t9p9Z6GA1r6NHAwXN3pLVeqrUepbUeFR0dfUkFu8vKOP2vf+E+c+aSPl8IIfyV2YMxm4F+Sqk+wGHgFuDHTQcopSKBisY19nuAAq21w8u1AlBdXIxr1WrKCwoIHz8ea2oq5qio1jiUEEL4lIsGuta6Tin1AJALmIBXtdY7lVL3NW5fAgwA3lBKuYFdwN2tVXDY6NEE9e6NIzcP19p1uNatI3zcOGxpaZg7d26twwohRLuntD53ObxtjBo1ShcWFl7WPupOnsSZl4dr/XrQED5mDNa0VAK7dvVSlUII0b4opYq01qOa3ebLgf6dutOnceYtp3zdWrS7nrDE0djS0wnsJldOCiH8i98H+nfcZ87gWL6c8oK16Lo6wkaNwpaRTmCPHl49jhBCGKXDBPp33A4HzuX5uNasQdfWEjpiOLb0DIJizr18XgghfEuHC/TvuJ1OnCtW4Fq1Gl1dTejw4dgy0gnq1evinyyEEO1Qhw3077hd5bhWrcS1ahX1FZWEDh2CLT2doN692+T4QgjhLR0+0L9TX1GBc9UqXCtWUl9RQcigQdgyMwnu26dN6xBCiEslgX6O+spKXGvW4FyeT315OcED+hORmUnw1VcbUo8QQnhKAv086qur/xPsTifB8fHYMjIIjuuHUs21sBFCCGNJoF9EfU0N5WvX4szLw13mILjf1Q3B3r+/BLsQol2RQPeQrqnBtW59Q7CfOUNQ377YMjIIGTRQgl0I0S5IoLeQrq2lfMMGHDm5uE+dIig2Ftu1mYQkJEiwCyEMJYF+iXRdHeUbN+HItuM+eYrAXr2wZaQTOmyYBLsQwhAS6JdJ19VRsXkzDns2daWlBPbs2RDsI0ZIsAsh2tSFAt2TfugdnjKbCR83jrDERCo2F+LIzubk317G3KM7ERkZhI4ciQrw5FkhQgjReuQM/RLo+noqi4oos9upO/pvzN26YUtPJ2z0KJTJZHR5Qgg/JksurURrTeXWrTiy7NQePow5OhpbehphiYkos/znRwjhfRLorUxrTdX27ZTZ7dR+ewhT507Y0tIJHzdWgl0I4VUS6G1Ea03Vjh04Ps2i5ptvMEVFYUtLJfyaa1CBgUaXJ4TwAxLobUxrTdWuXTiy7NQcOIApMhJrSgqWCeNRQUFGlyeE8GES6AbRWlO9dy+OrCyq9+0nwGbFlpJC+MSJBAQHG12eEMIHyWWLBlFKEdK/PyH9+1NVXIwjy86Z997HkZuHddo0LJMnSbALIbxGAr2NhMTFERIXR/X+/TjsdsqWLcOZl4d12lQskycTEBpqdIlCCB8nSy4GqT5wEEe2naovdxAQFopl6lSsyckEhIUZXZoQoh2TNfR2rOabb3DY7VRu/wIVGoJ1yhQsyVMwWcKNLk0I0Q5JoPuAmkOHcNizqdy6FRUcjCV5MtZp0zBZLEaXJoRoRyTQfUhNyWGcOdlUFG1BBQVhSZqIdfp0TDab0aUJIdoBCXQfVHv0KI7sHCo2b25oDpY0Edv06ZgiI40uTQhhIAl0H1Z77BjOnBzKN32OMgUQPn4C1tQUzFFRRpcmhDCABLofqD1+HGdOLuWbNoGC8GuuwZaairlzZ6NLE0K0IQl0P1J38iSO3FzKP/sMNISPHYstLRVzdLTRpQkh2oAEuh+qO30aZ24e5evXod31hI9JxJqWTmC3rkaXJoRoRZcd6EqpNOA5wAS8rLV+8pztEcCbwJU03H36rNb6tQvtUwLdO9xnzuBYvpzygrXoujrCEhOxpacR2L270aWJdqq2tpaSkhKqqqqMLkVcQEhICDExMQSe06n1sgJdKWUCioHpQAmwGbhVa72ryZjfABFa64eVUtHAXqC71rrmfPuVQPcud1kZzvx8XGsK0LW1hI0cgS09ncCePY0uTbQzBw8exGq10rlzZ3kmbjultebkyZM4nU769OnzvW2X25wrEdivtT7QuLN3gJnAriZjNGBVDT8dFuAUUNfyaYhLZYqIIPKGG7CmpODMX4Fr9WoqCosIHT4cW2YGQTExRpco2omqqip69+4tYd6OKaXo3LkzpaWlLfo8TwK9J3CoyesSYMw5Y14APgaOAFbgZq11fTNFzgPmAVx55ZUtKlR4xmS1Ejl7Ftbp03GtWolz5Uoqt24ldOgQbBkZBMXGGl2iaAckzNu/S/keefKo+ub2eu46TSqwDbgCGAa8oJT6wa2NWuulWutRWutR0XJVRqsyWcKJuO46rli0CNt111K9bx/H/vQkpS++SPWBg0aXJ4RoBZ4EegnQq8nrGBrOxJu6E/hAN9gPHAT6e6dEcTkCwsKIyMykx6JFRMycQc2Bgxx/+mlKn/9fqr/6yujyhBBe5MmSy2agn1KqD3AYuAX48TljvgWmAmuVUt2AeOCANwsVlycgNBRbejqW5GRcawpw5udz/JlnCY6Px5aZQUhcnNElig7s8ccfx2Kx4HA4SEpKYtq0aUaXRO/evSksLKRLly7NbrdYLLhcrjau6sIuGuha6zql1ANALg2XLb6qtd6plLqvcfsS4AngdaXUlzQs0TystT7RinWLSxQQEoItNQXL5EmUr12LIy+P0sV/JrhfP2zXZhIcFyfrqx3I259/y7enKry6zys7hXFr4qX9juwPf/iDV2uBhitGtNYEBHiyIOHbPJqh1tqutY7TWl+ltV7U+N6SxjBHa31Ea52itR6stU7QWr/ZmkWLyxcQHIx12jSuWLiQyJtuou74cUr//BeOP/sslTt3YtQNZ6LjWLRoEfHx8UybNo29e/cCMHfuXN577z0AHnnkEQYOHMiQIUN46KGHADh27BizZ89m6NChDB06lM8++6zZfX/99dcMGDCA+fPnM2LECA4dOsQzzzzD6NGjGTJkCI899tjZsbNmzWLkyJEMGjSIpUuXtngeWmt++ctfkpCQwODBg3n33XcBOHr0KElJSQwbNoyEhATWrl2L2+1m7ty5Z8f++c9/bvHxLkQeQdfBqaAgrFOSsUwYT/mGDThycjnxvy8Q1Lt3w1JMQoKcsfuxSz2TvlxFRUW88847bN26lbq6OkaMGMHIkSPPbj916hTLli1jz549KKU4c+YMAD/72c+YNGkSy5Ytw+12X3DJY+/evbz22mv89a9/JS8vj3379vH555+jtWbGjBkUFBSQlJTEq6++SqdOnaisrGT06NHccMMNdG5Bj6QPPviAbdu2sX37dk6cOMHo0aNJSkriH//4B6mpqfz2t7/F7XZTUVHBtm3bOHz4MDt27AA4Oy9vkUAXQEOwWyZNInz8eMo3bMSRk82JF/9K4JW9iMjMJGTIEAl24TVr165l9uzZhDU+cnHGjBnf226z2QgJCeGee+4hMzOTa6+9FoCVK1fyxhtvAGAymYiIiDjvMWJjYxk7diwAeXl55OXlMXz4cABcLhf79u0jKSmJ559/nmXLlgFw6NAh9u3b16JAX7duHbfeeismk4lu3boxadIkNm/ezOjRo7nrrruora1l1qxZDBs2jL59+3LgwAEefPBBMjMzSUlJ8fg4nvD/RSXRIspsxjJxAj1+/3s63X4burKKEy8t4diiP1KxZassxQivudAJgtls5vPPP+eGG27gww8/JC0trcX7Dw//z2Mctdb8+te/Ztu2bWzbto39+/dz9913s3r1avLz89mwYQPbt29n+PDhLW6JcL6/E0lJSRQUFNCzZ09uu+023njjDaKioti+fTuTJ0/mxRdf5J577mnxvC5EAl00S5nNhF9zDd0ff4xOc+eia2s5uXQpx554gorNm9H1P7hvTAiPJSUlsWzZMiorK3E6nXzyySff2+5yuSgrKyMjI4O//OUvbNu2DYCpU6fy0ksvAeB2u3E4HB4dLzU1lVdfffXsEs3hw4c5fvw4ZWVlREVFERYWxp49e9i4ceMlzeXdd9/F7XZTWlpKQUEBiYmJfPPNN3Tt2pV7772Xu+++my1btnDixAnq6+u54YYbeOKJJ9iyZUuLj3chsuQiLkiZTISPHUNY4mgqi4oos9s5+cqrmD/NwpaeTtjoUSiTyegyhY8ZMWIEN998M8OGDSM2NpaJEyd+b7vT6WTmzJlUVVWhtT77y8PnnnuOefPm8corr2AymXjppZcYN27cRY+XkpLC7t27z461WCy8+eabpKWlsWTJEoYMGUJ8fPzZJZqWmD17Nhs2bGDo0KEopXj66afp3r07//d//8czzzxDYGAgFouFN954g8OHD3PnnXdS33hC9Kc//anFx7sQaZ8rWkRrTeWWLTjs2dQePow5OhpbRjpho0ejzHJ+4At2797NgAEDjC5DeKC579XlNucS4iylFGEjRxI6YgSV27bhsGdz6v/eoCwrC1taOuFjx0iwC2EQ+ZsnLolSirDhwwkdNoyqHTtwfJrF6TffxGG3Y0tLJXzcONQ5fZyF8LaTJ08yderUH7y/YsWKFl2pYtT+vU0CXVwWpRShgwcTkpBA1c5dOLKyOP2Pt3HYs7GmpmAZPx4VFGR0mcJPde7c+ewvTH1x/94mgS68QilFaMIgQgYNpHrPHhxZWZx59584c3KxpkwnfOJEAiTYhWhVEujCq5RShAwYQHD//lQX78Nht3PmX+/hyMnFOn0alkmTCAgONrpMIfySBLpoFUopQuLjCImPo3rfPsrsdso+WIYzN+8/wR4aanSZQvgVubFItLrgfv3o+vOf0/VXvySod2/KPvyIo7/9HWVZWdRXeLfTn/BNjz/+OM8++ywLFiwgPz/fK/t8/vnnGTBgAHPmzDnvmNdff50HHnjgso4zefJk2ssl2HKGLtpMcN++RD/4ADVff40jOxvHJ5/iWrECS3IyluQpmCzhF9+J8GvebJ/717/+lezs7B88ZNmfSaCLNhfUuzdd7r+fmkOHcNizcWTZca5YiTV5MpapUzFZLEaX2HEUvQ6nv/buPqN6w8i5Fx22aNEi3njjDXr16kV0dDQjR45k7ty5XHvttdx444088sgjfPzxx5jNZlJSUnj22Wc5duwY9913HwcONDw/56WXXuKaa675wb6/GzNjxgzuuusuxo8fz3//939TWVlJaGgor732GvHx8d/7nKysLBYuXMgnn3zCli1beOyxx6iuruaqq67itddew+LBz+Xbb7/NH//4R7TWZGZm8tRTT+F2u7n77rspLCxEKcVdd93FL37xC55//nmWLFmC2Wxm4MCBvPPOOx59eS9EAl0YJqhXL7r81zxqSg7jyLbjyMnFuXIVlkmTsE6fhslqNbpE0Upau33ukiVLyMnJYdWqVXTp0gWHw0FBQQFms5n8/Hx+85vf8P77758dv2zZMhYvXozdbsftdrNw4ULy8/MJDw/nqaeeYvHixSxYsOCCczpy5AgPP/wwRUVFREVFkZKSwocffkivXr2abZn75JNPcvDgQYKDg73WRlcCXRguKKYnXe69l9qjR3HYs3EuX45r1Sosk5KwTpuGKTLS6BL9lwdn0q2hLdrnNlVWVsYdd9zBvn37UEpRW1t7dtuqVasoLCwkLy8Pm83Gp59+yq5duxg/fjwANTU1HvWL2bx5M5MnTyY6OhqAOXPmUFBQwKOPPtpsy9whQ4YwZ84cZs2axaxZszyax8XIL0VFuxHYowed776L7o8/RujIEThXruLoows4/e4/qTt92ujyhJe1dvvcph599FGSk5PZsWMHn3zyyfda5Pbt2xen00lxcTHQ0K9o+vTpZ1vt7tq1i1deeeWixzhfX6zztczNysripz/9KUVFRYwcOZK6urrLmiNIoIt2KLBbNzrPnUv3xx8jbPRoXAUFHH30UU6//TZ1p04ZXZ7wgrZun1tWVkbPnj2BhitbmoqNjeWDDz7g9ttvZ+fOnYwdO5b169ezf/9+ACoqKs6G/YWMGTOGNWvWcOLECdxuN2+//TaTJk1qtmVufX09hw4dIjk5maeffpozZ8545YHTsuQi2q3Arl3pdPtt2DIzcOTk4Fq/Htf69YSPHYctLRXzeZ7GLtq/tm6f+6tf/Yo77riDxYsXM2XKlB9sj4+P56233uJHP/oRn3zyCa+//jq33nor1dXVACxcuJC4uLgLHqNHjx786U9/Ijk5Ga01GRkZzJw5k+3bt/+gZa7b7eYnP/kJZWVlaK35xS9+QaQXlhalfa7wGXWnT+PMzcW1bh3Ua8LHjsGalkZg165Gl+ZTpH2u75D2ucJvmaOiiLrlFqypqTiXL6d87TrKN2wkLDERW0Y6gd26GV2iEIaSQBc+xxwVRdRNN2FLTcWZn49rTQEVn39O2KiR2NLTCbziCqNLFG2kLdvbzp49m4MHD37vvaeeeorU1FSvHudySKALn2WKiCDyhhuwpqTgzF+Ba/VqKgqLCB0xHFt6BkExPY0uUbSytmxvu2zZsjY5zuWQQBc+z2S1Ejl7Ftbp03CtWIFz1Woqi7YQOmwotowMgq680ugShWgTEujCb5gsFiJmzsQ6bRrOlatwrVrJsW3bCRmcgC0jg+AO1NNDdEwS6MLvBISHE3HdtVinTsG1Zg3O/BUcf+ppQgYNwpaZQXDfvkaXKESrkEAXfisgLAxbejqW5GRcq9fgzM/n+NPPEDygPxEZGQT362d0iUJ4ldwpKvxeQEgItrRUeixaSMQN11N7+DDH/2cxxxf/maq9e897y7bwTV9//TUJCQmteowXXniBq6++GqUUJ06caNVjtYQEuugwAoKDsU2fTo+FC4m86UfUHTtG6Z//Qun//A9Vu3dLsAuPjR8/nvz8fGJjY40u5Xs8WnJRSqUBzwEm4GWt9ZPnbP8l8N1jQczAACBaay2NN0S7ExAUhHXKFCwTJuD67DOcuXmUPvc8QX36YMvMJGTQwAs2jvIn7xW/R4mzxKv7jLHGcGPcjRcd98QTT/DWW2/Rq1cvunTpwsiRI5k9ezY//elPKS0tJSwsjL/97W/079+fuXPnYrPZKCws5N///jdPP/00N9548WNUVVVx//33U1hYiNlsZvHixSQnJ7Nz507uvPNOampqqK+v5/333+eKK67gpptuoqSkBLfbzaOPPsrNN9/c7H6HDx/e4q9LW7hooCulTMCLwHSgBNislPpYa73ruzFa62eAZxrHXwf8QsJctHcqKAjr5MlYxo+nfONGHDk5nHjhBYJiY7FlpBMyZEiHCfa2VlhYyPvvv/+Dfujz5s1jyZIl9OvXj02bNjF//nxWrlwJwNGjR1m3bh179uxhxowZHgX6iy++CMCXX37Jnj17SElJobi4mCVLlvDzn/+cOXPmUFNTg9vtxm63c8UVV5CVlQU0NPTyNZ6coScC+7XWBwCUUu8AM4Fd5xl/K/C2d8oTovWpwEAsEycSPm4c5Zs24czJ4cRLSwjs1Qtbejqhw4f5bbB7cibdGtatW8fMmTMJbXxQ+HXXXUdVVRWfffYZP/rRj86O+645FsCsWbMICAhg4MCBHDt2zOPjPPjggwD079+f2NhYiouLGTduHIsWLaKkpITrr7+efv36MXjwYB566CEefvhhrr322h80DPMFnqyh9wQONXld0vjeDyilwoA04P3mtgvRnimzGcv48XR//HE6zb0DXV3NyaVLOfbEQiqKitCN3fLE5Wvu9xX19fVERkae7UO+bds2du/efXZ7cHDwBT/f0+MA/PjHP+bjjz8mNDSU1NRUVq5cSVxcHEVFRQwePJhf//rXXn2+aVvxJNCbOzU531fzOmD9+ZZblFLzlFKFSqnC0tJST2sUok0pk4nwsWPp/vhjdLrrTnS9m5N/e5l/P/EE5Z9/LsHuBRMmTDj7oAmXy0VWVhZhYWH06dOHf/3rX0BDGG/fvv2yjpOUlMRbb70FQHFxMd9++y3x8fEcOHCAvn378rOf/YwZM2bwxRdfcOTIEcLCwvjJT37CQw89xJYtWy57nm3Nk0AvAXo1eR0DHDnP2Fu4wHKL1nqp1nqU1nrUd49pEqK9UgEBhCcm0n3BAjrfew9KBXDq1df49+//QPnGjWi32+gSfdbo0aOZMWMGQ4cO5frrr2fUqFFERETw1ltv8corrzB06FAGDRrERx99dFnHmT9/Pm63m8GDB3PzzTfz+uuvExwczLvvvktCQgLDhg1jz5493H777Xz55ZckJiYybNgwFi1axO9+97vz7vf5558nJiaGkpIShgwZcvYpREa7aD90pZQZKAamAoeBzcCPtdY7zxkXARwEemmtyy92YOmHLnyN1prKrdtw2O3UlpRgju6CNS2N8DFjUGbfuUevvfRDd7lcWCwWKioqSEpKYunSpYwYMcLostoVr/dD11rXKaUeAHJpuGzxVa31TqXUfY3blzQOnQ3keRLmQvgipRRhI4YTOnwYVV98gcNu5/Tf38Rht2NLSyN87FhUYKDRZfqMefPmsWvXLqqqqrjjjjskzL1AnlgkxCXSWlO1cxeOrCxqDh7EFBWFNTUFyzXXoIKCjC7vvNrLGfrl+vLLL7ntttu+915wcDCbNm3y2jGM7oHe0jN0CXQhLpPWmuo9eyj79FNqvjqAKSKiIdgnTGiXwe4vgd4RyCPohGhjSilCBgwguH9/qouLcWTZOfPPf+HIycE6fTqWpCQCmlxyJ0RrkUAXwkuUUoTExxMSH09VcTGO7GzK3v8AZ24e1mnTsEyeREBIiNFlCj8mgS5EKwiJiyMkLo7qr77CYc+m7MMPcS5fjnXaVCyTJhEQFmZ0icIPSaAL0YqCr7qK6AcfoPrgwYYz9o8+xrl8OZbkKVinJBMQHm50icKPSPtcIdpAcJ8+RM+fT7ff/JrguHgcWVkc+d3vKPvoI9wuudLXm9qiH/qcOXOIj48nISGBu+66i9ra2lY9nqck0IVoQ0FXXkmX+/6Lbr/7HSEDB+LIyeXob3/LmWUf4nY6jS5PeGjOnDns2bOHL7/8ksrKSl5++WWjSwJkyUUIQwTF9KTLvfdSe+QIjuxsnHl5uFatwjIpCeu0aZgiItqkjtP//Ce1h7zbDz2wVwxRN9100XG+3A89IyPj7MeJiYmUlHj3a3ipJNCFMFDgFVfQ+e67sWVm4sjOwZm/AtfqNYRPnIAtJQVTZKTRJbYKf+mHXltby9///neee+65y/hqeI8EuhDtQGD37nS+cy62zAycOTm4Vq+hfO1awsePx5qaijkqqlWO68mZdGvwl37o8+fPJykpqd30Tpc1dCHakcCuXel0++30+MPvCRszFte6dRx99FFOvfUWdSdPGl2e1/hDP/Tf//73lJaWsnjxYo9qaQsS6EK0Q+YuXej0kzn0+MMfGh6Rt2EDRxc8xqk3/k7t8eNGl3fZfL0f+ssvv0xubi5vv/02AQHtJ0ZlyUWIdszcqRNRt96KNS0NZ95yyteto3zjRsISR2NLTyewWzejS7wkTfuhx8bGfq8f+v3338/ChQupra3llltuYejQoZd8nPnz53PfffcxePBgzGbz9/qhv/nmmwQGBtK9e3cWLFjA5s2b+eUvf0lAQACBgYG89NJL593vfffdR2xsLOPGjQPg+uuvZ8GCBZdcp7dIcy4hfIj7zBmc+fm41hSg6+oIGzUKW0Y6gT16eLyP9tKcS/qhX5w05xLCj5kiI4m88Uasqak4l+fjWrOGisJCQkcMx5aeQVBMs4/7bZekH7r3SaAL4YNMViuR18/GmjId14oVOFetprJoC6HDh2PLSCeoV6+L78Rg//jHPy7r8ztCP/SWkiUXIfyA21WOa9VKXKtWUV9RSejQIdjS0wnq3fsHY3fv3k3//v1Rqrnnv4v2QmvNnj17ZMlFiI7GZAkn4rrrsE6dimv1apz5Kzi2/SlCEhKwZWQQ3LfP2bEhISGcPHmSzp07S6i3U1prTp48SUgL2y3LGboQfqi+shLXmgKcy5dTX15O8ID+RGRmEnz11dTW1lJSUkJVVZXRZYoLCAkJISYmhsBznlMrj6ATooOqr67GVVCAM2859U4nwfHxDWfscf3k7NxHSaAL0cHV19RQvnYtzrw83GUOgvtd3RDsspbucyTQhRAA6JoaXOvX48zNw33mDEF9+2LLyCBk0EAJdh8hgS6E+B5dW0v5hg04cnJxnzpFUGwstmszCUlIkGBv5yTQhRDN0nV1lG/ahCM7G/eJkwT26oUtI53QYcMk2NspCXQhxAXpujoqNm/GYc+mrrSUwJ49G4J9xAgJ9nZGrkMXQlyQMpsJHzeOsMREKgqLcGTbOfm3lzH36E5ERgahI0ei2lFXQdE8OUMXQvyArq+ncssWHHY7tUeOYu7WDVt6OmGjR6FMJqPL69BkyUUIcUm01lRu3YYjK4vaw4cxR0djS08jLDERZZb/4BtBAl0IcVm01lR98QVlWVnUfnsIU+dO2NLSCR83VoK9jUmgCyG8QmtN1Y6dOLKyqPn6a0xRUdjSUgm/5hrUObeoi9YhgS6E8CqtNdW7d1OWlUXNVwcwRUZiTUnBMmE8KijI6PL82mVf5aKUSgOeA0zAy1rrJ5sZMxn4CxAInNBaT7rEeoUQ7ZxSipCBAwkeMIDq4mIcn2Zx5p//xJGTjS0lhfCJEwlo8lBn0TYueoaulDIBxcB0oATYDNyqtd7VZEwk8BmQprX+VinVVWt9wSfZyhm6EP6lqrgYh91O9Z69BFitWKdNwzJ5kgS7l13uGXoisF9rfaBxZ+8AM4FdTcb8GPhAa/0twMXCXAjhf0Li4giJi6P6q69wZNkpW7YMZ14e1mlTsUyeTEBoqNEl+j1P7hToCRxq8rqk8b2m4oAopdRqpVSRUur25naklJqnlCpUShWWlpZeWsVCiHYt+KqriP7Zg3T91a8I6tuHso8+5uhvf0tZVhb1FRVGl+fXPDlDb+6+33PXaczASGAqEApsUEpt1FoXf++TtF4KLIWGJZeWlyuE8BXBffsQ/dOfUvPNNzjsdhyffIozPx/rlClYkqdgsoQbXaLf8STQS4CmT5yNAY40M+aE1rocKFdKFQBDaVh7F0J0YEGxsXS5/35qSkpw2LNxZNlx5q/AkjwZ67RpmCwWo0v0G54E+magn1KqD3AYuIWGNfOmPgJeUEqZgSBgDPBnbxYqhPBtQTExdJl3L7WHD+PIycGZm4dr1WosSROxTp+OyWYzukSfd9FA11rXKaUeAHJpuGzxVa31TqXUfY3bl2itdyulcoAvgHoaLm3c0ZqFCyF8U2DPnnS++25smZk4snNw5q/AtXoN4UkTsU2fjiky0ugSfZbcWCSEMFTtseM4c7Ip3/Q5yhRA+PgJWFNTMEdFGV1auyR3igoh2r260lIcObmUb9wICsKvuQZbairmzp2NLq1dkUAXQviMupMncebl4Vq/HjSEjx2LLS0Vc3S00aW1CxLoQgifU3f6NM685ZSvW4t21xM+JhFrWjqB3boaXZqhJNCFED7LfeYMjuXLKS9Yi66rIywxEVt6GoHduxtdmiEk0IUQPs/tcOBcno9rzRp0bS1hI0dgS08nsOe5N677Nwl0IYTfcDudOFeswLVqNbq6mtDhw7FlZhAUE2N0aW1CHhIthPAbJquVyFmzsE6bjmvVSpwrV1K5dSuhQ4dgy8ggKDbW6BINI2foQgifVl9RgXPVKlwrVlBfUUnI4ARs6RkE9+1jdGmtQpZchBB+r76yEteaNTiX51NfXk7IwIHYMjMIvuoqo0vzKgl0IUSHUV9d/Z9gdzoJ7h+PLSODkLg4o0vzCgl0IUSHU19dTfm6dTjz8nCXOQju1w/btZkEx8WhVHNdwX2DBLoQosPSNTW41q3HmZuLu6yMoKv6NpyxDxzok8EugS6E6PB0TQ3lGzbgyMnFffo0Qb17Y8vMICQhwaeCXQJdCCEa6bo6yjduwpFtx33yFIFX9iIiM5OQIUN8Itgl0IUQ4hy6ro6KzZtx2LOpKy0lMCYGW0YGocOHtetglxuLhBDiHMpsJnzcOMISE6nYXIgjO5uTS5cSeEUPbOnphI4ciQoIMLrMFpEzdCGEAHR9PZVFRZTZ7dQd/Tfmbt2wpacTNnoUymQyuryzZMlFCCE8pLWmcutWHFl2ag8fxhwdjS0jnbDRo1Fm4xc1JNCFEKKFtNZUbd9OWZad2kOHMHXpjC0tnfCxYwwNdgl0IYS4RFprqnbswPFpFjXffIOpUydsaamEjxuHCgxs83ok0IUQ4jJpranatQtHlp2aAwcwRUZiTU3BMn48KiiozeqQq1yEEOIyKaUIHTSIkIEDqd67F0dWFmfe/SfOnFysKdMJnziRgDYM9uZIoAshRAsopQjp35+Q/v2pKi7GkWXnzL/ew5GTi3X6NCyTJhEQHGxIbRLoQghxiULi4giJi6N6/34cdjtlHyzDmZv3n2APDW3TemQNXQghvKT6wEEcdjtVO3YQEBaGZeoUrMnJBISFee0Y8ktRIYRoQzVff40jO5vK7V8QEBaKJTkZS/IUTJbwy963BLoQQhig5tAhHPZsKrduRYWEYE2ejGXqVEwWyyXvUwJdCCEMVFNyGGdONhVFW1BBQUTMnIl1SvIl7UsuWxRCCAMFxfSk8z33YMs8iiM7B1OErVWOI4EuhBBtJLBHDzrfdWer7d+3ekMKIYQ4L48CXSmVppTaq5Tar5R6pJntk5VSZUqpbY1/Fni/VCGEEBdy0SUXpZQJeBGYDpQAm5VSH2utd50zdK3W+tpWqFEIIYQHPDlDTwT2a60PaK1rgHeAma1blhBCiJbyJNB7AoeavC5pfO9c45RS25VS2UqpQc3tSCk1TylVqJQqLC0tvYRyhRBCnI8ngd7c01LPvXh9CxCrtR4K/C/wYXM70lov1VqP0lqPio6OblGhQgghLsyTQC8BejV5HQMcaTpAa+3QWrsaP7YDgUqpLl6rUgghxEV5EuibgX5KqT5KqSDgFuDjpgOUUt2VUqrx48TG/Z70drFCCCHO76JXuWit65RSDwC5gAl4VWu9Uyl1X+P2JcCNwP1KqTqgErhFG9VTQAghOijp5SKEED7kQr1c5E5RIYTwExLoQgjhJyTQhRDCT/hct8WPd33O37a/bXQZQghxySb3msD/m3CD1/frc4FuCwmje9gVRpchhBCXrHNoRKvs1+cCfXLfBCb3TTC6DCGEaHdkDV0IIfyEBLoQQvgJCXQhhPATEuhCCOEnJNCFEMJPSKALIYSfkEAXQgg/IYEuhBB+wrD2uUqpUuCbS/z0LsAJL5bjC2TOHYPMuWO4nDnHaq2bfYanYYF+OZRShefrB+yvZM4dg8y5Y2itOcuSixBC+AkJdCGE8BO+GuhLjS7AADLnjkHm3DG0ypx9cg1dCCHED/nqGboQQohzSKALIYSfaNeBrpRKU0rtVUrtV0o90sx2pZR6vnH7F0qpEUbU6U0ezHlO41y/UEp9ppQaakSd3nSxOTcZN1op5VZK3diW9bUGT+aslJqslNqmlNqplFrT1jV6mwc/2xFKqU+UUtsb53ynEXV6i1LqVaXUcaXUjvNs935+aa3b5R/ABHwF9AWCgO3AwHPGZADZgALGApuMrrsN5nwNENX4cXpHmHOTcSsBO3Cj0XW3wfc5EtgFXNn4uqvRdbfBnH8DPNX4cTRwCggyuvbLmHMSMALYcZ7tXs+v9nyGngjs11of0FrXAO8AM88ZMxN4QzfYCEQqpXq0daFedNE5a60/01qfbny5EYhp4xq9zZPvM8CDwPvA8bYsrpV4MucfAx9orb8F0Fr7+rw9mbMGrEopBVhoCPS6ti3Te7TWBTTM4Xy8nl/tOdB7AoeavC5pfK+lY3xJS+dzNw3/wvuyi85ZKdUTmA0sacO6WpMn3+c4IEoptVopVaSUur3Nqmsdnsz5BWAAcAT4Evi51rq+bcozhNfzqz0/JFo1896511h6MsaXeDwfpVQyDYE+oVUran2ezPkvwMNaa3fDyZvP82TOZmAkMBUIBTYopTZqrYtbu7hW4smcU4FtwBTgKmC5Umqt1trRyrUZxev51Z4DvQTo1eR1DA3/crd0jC/xaD5KqSHAy0C61vpkG9XWWjyZ8yjgncYw7wJkKKXqtNYftkmF3ufpz/YJrXU5UK6UKgCGAr4a6J7M+U7gSd2wwLxfKXUQ6A983jYltjmv51d7XnLZDPRTSvVRSgUBtwAfnzPmY+D2xt8WjwXKtNZH27pQL7ronJVSVwIfALf58NlaUxeds9a6j9a6t9a6N/AeMN+Hwxw8+9n+CJiolDIrpcKAMcDuNq7TmzyZ87c0/I8EpVQ3IB440KZVti2v51e7PUPXWtcppR4Acmn4DfmrWuudSqn7GrcvoeGKhwxgP1BBw7/wPsvDOS8AOgN/bTxjrdM+3KnOwzn7FU/mrLXerZTKAb4A6oGXtdbNXv7mCzz8Pj8BvK6U+pKG5YiHtdY+21ZXKfU2MBnoopQqAR4DAqH18ktu/RdCCD/RnpdchBBCtIAEuhBC+AkJdCGE8BMS6EII4Sck0IUQwk9IoAshhJ+QQBdCCD/x/wF9yMsc9EtEIwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "BATCH_SIZE = 2\n",
    "BATCH_GENERATOR = BatchGenerator(BATCH_SIZE,'./NatureTrim', 2, 512,512)\n",
    "EPOCHS = 6\n",
    "STEPS_PER_EPOCH = 1\n",
    "OPTIMIZER = keras.optimizers.Adam(1e-4)\n",
    "GENERATOR = UNET(OPTIMIZER)\n",
    "DISCRIMINATOR = PatchDiscriminatorLSTM(OPTIMIZER,70)\n",
    "\n",
    "train_loop(GENERATOR, DISCRIMINATOR, BATCH_SIZE, EPOCHS, STEPS_PER_EPOCH, BATCH_GENERATOR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
